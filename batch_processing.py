PROCESAMIENTO BATCH - ACCIDENTALIDAD VIAL BOGOT√Å
An√°lisis de datos hist√≥ricos con Apache Spark
Autor: UNAD - Ingenier√≠a de Sistemas  
Wilson Stiven Rojas Diaz
Curso: Big Data 
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *

print("="*80)
print("AN√ÅLISIS BATCH - ACCIDENTALIDAD VIAL BOGOT√Å")
print("="*80)

# Crear sesi√≥n de Spark
spark = SparkSession.builder \
    .appName("AccidentalidadBogota_Batch") \
    .config("spark.master", "local[*]") \
    .getOrCreate()

spark.sparkContext.setLogLevel("ERROR")

print("\n[1/5] Cargando datos...")

# Definir esquema
schema = StructType([
    StructField("FECHA", StringType()),
    StructField("HORA", StringType()),
    StructField("DIA_SEMANA", StringType()),
    StructField("LOCALIDAD", StringType()),
    StructField("DIRECCION", StringType()),
    StructField("LATITUD", DoubleType()),
    StructField("LONGITUD", DoubleType()),
    StructField("CLASE_ACCIDENTE", StringType()),
    StructField("GRAVEDAD", StringType()),
    StructField("NUM_HERIDOS", IntegerType()),
    StructField("NUM_MUERTOS", IntegerType()),
    StructField("TIPO_VEHICULO", StringType()),
    StructField("CLIMA", StringType()),
    StructField("ESTADO_VIA", StringType()),
    StructField("CAUSA_PROBABLE", StringType()),
    StructField("GENERO", StringType()),
    StructField("EDAD", IntegerType()),
    StructField("TIPO_SERVICIO", StringType())
])

# Leer archivo CSV
# Aseg√∫rate de tener el archivo en la ruta correcta
df = spark.read.csv(
    "data/amazon.xlsx",  # Cambia esto a tu archivo
    header=True,
    schema=schema
)

total_records = df.count()
print(f"‚úì Datos cargados: {total_records:,} registros")

print("\n[2/5] Limpieza y transformaci√≥n...")

# Eliminar duplicados
df = df.dropDuplicates()

# Manejar nulos
df = df.na.fill({
    "NUM_HERIDOS": 0,
    "NUM_MUERTOS": 0,
    "EDAD": 35,
    "CLIMA": "Desconocido"
})

# Convertir fecha
df = df.withColumn("FECHA", to_date(col("FECHA")))
df = df.withColumn("ANIO", year(col("FECHA")))
df = df.withColumn("MES", month(col("FECHA")))

# Extraer hora del d√≠a
df = df.withColumn("HORA_DIA", hour(to_timestamp(col("HORA"), "HH:mm:ss")))

# Clasificar franja horaria
df = df.withColumn("FRANJA_HORARIA",
    when((col("HORA_DIA") >= 6) & (col("HORA_DIA") < 12), "Ma√±ana")
    .when((col("HORA_DIA") >= 12) & (col("HORA_DIA") < 18), "Tarde")
    .when((col("HORA_DIA") >= 18) & (col("HORA_DIA") < 24), "Noche")
    .otherwise("Madrugada")
)

# Total v√≠ctimas
df = df.withColumn("TOTAL_VICTIMAS", col("NUM_HERIDOS") + col("NUM_MUERTOS"))

# Grupo de edad
df = df.withColumn("GRUPO_EDAD",
    when(col("EDAD") < 25, "Joven (18-24)")
    .when((col("EDAD") >= 25) & (col("EDAD") < 40), "Adulto (25-39)")
    .when((col("EDAD") >= 40) & (col("EDAD") < 60), "Adulto Mayor (40-59)")
    .otherwise("Senior (60+)")
)

# Cache para mejor rendimiento
df.cache()
print("‚úì Transformaciones completadas")
print(f"  Registros despu√©s de limpieza: {df.count():,}")

print("\n[3/5] An√°lisis con Spark SQL...")

# Crear vista temporal
df.createOrReplaceTempView("accidentes")

# AN√ÅLISIS 1: Top 10 localidades
print("\nüìç TOP 10 LOCALIDADES CON M√ÅS ACCIDENTES:")
print("-" * 80)
top_localidades = spark.sql("""
    SELECT 
        LOCALIDAD,
        COUNT(*) as total_accidentes,
        SUM(NUM_HERIDOS) as total_heridos,
        SUM(NUM_MUERTOS) as total_muertos,
        ROUND(AVG(TOTAL_VICTIMAS), 2) as promedio_victimas
    FROM accidentes
    GROUP BY LOCALIDAD
    ORDER BY total_accidentes DESC
    LIMIT 10
""")
top_localidades.show(truncate=False)

# AN√ÅLISIS 2: Distribuci√≥n por franja horaria
print("\n‚è∞ DISTRIBUCI√ìN POR FRANJA HORARIA:")
print("-" * 80)
franja = spark.sql("""
    SELECT 
        FRANJA_HORARIA,
        COUNT(*) as total_accidentes,
        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as porcentaje
    FROM accidentes
    GROUP BY FRANJA_HORARIA
    ORDER BY total_accidentes DESC
""")
franja.show()

# AN√ÅLISIS 3: Accidentes por d√≠a de la semana
print("\nüìÖ ACCIDENTES POR D√çA DE LA SEMANA:")
print("-" * 80)
dias = spark.sql("""
    SELECT 
        DIA_SEMANA,
        COUNT(*) as total_accidentes,
        SUM(NUM_MUERTOS) as total_muertos
    FROM accidentes
    GROUP BY DIA_SEMANA
    ORDER BY total_accidentes DESC
""")
dias.show()

# AN√ÅLISIS 4: Por tipo de veh√≠culo
print("\nüöó ACCIDENTES POR TIPO DE VEH√çCULO:")
print("-" * 80)
vehiculo = spark.sql("""
    SELECT 
        TIPO_VEHICULO,
        COUNT(*) as total_accidentes,
        SUM(NUM_MUERTOS) as total_muertos,
        ROUND(SUM(NUM_MUERTOS) * 100.0 / COUNT(*), 2) as tasa_mortalidad
    FROM accidentes
    GROUP BY TIPO_VEHICULO
    ORDER BY total_accidentes DESC
""")
vehiculo.show()

# AN√ÅLISIS 5: Correlaci√≥n clima-accidentes
print("\nüå¶Ô∏è  ACCIDENTES POR CONDICI√ìN CLIM√ÅTICA:")
print("-" * 80)
clima = spark.sql("""
    SELECT 
        CLIMA,
        COUNT(*) as total_accidentes,
        SUM(NUM_HERIDOS) as heridos,
        SUM(NUM_MUERTOS) as muertos
    FROM accidentes
    GROUP BY CLIMA
    ORDER BY total_accidentes DESC
""")
clima.show()

# AN√ÅLISIS 6: Gravedad de accidentes
print("\n‚ö†Ô∏è  AN√ÅLISIS POR GRAVEDAD:")
print("-" * 80)
gravedad = spark.sql("""
    SELECT 
        GRAVEDAD,
        COUNT(*) as cantidad,
        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as porcentaje
    FROM accidentes
    GROUP BY GRAVEDAD
    ORDER BY cantidad DESC
""")
gravedad.show()

print("\n[4/5] An√°lisis con RDDs...")

# Convertir a RDD
rdd = df.rdd

# An√°lisis 1: Top 5 causas usando map/reduce
print("\nüîç TOP 5 CAUSAS DE ACCIDENTES (usando RDD):")
print("-" * 80)
causas = rdd \
    .map(lambda r: (r.CAUSA_PROBABLE, 1)) \
    .reduceByKey(lambda a, b: a + b) \
    .sortBy(lambda x: x[1], ascending=False) \
    .take(5)

for i, (causa, count) in enumerate(causas, 1):
    print(f"  {i}. {causa}: {count:,} accidentes")

# An√°lisis 2: Accidentes fatales por localidad
print("\nüíÄ ACCIDENTES FATALES POR LOCALIDAD (usando RDD):")
print("-" * 80)
fatales = rdd \
    .filter(lambda r: r.NUM_MUERTOS > 0) \
    .map(lambda r: (r.LOCALIDAD, r.NUM_MUERTOS)) \
    .reduceByKey(lambda a, b: a + b) \
    .sortBy(lambda x: x[1], ascending=False) \
    .take(5)

for i, (localidad, muertos) in enumerate(fatales, 1):
    print(f"  {i}. {localidad}: {muertos} fallecidos")

# An√°lisis 3: Promedio de edad por tipo de veh√≠culo
print("\nüë§ EDAD PROMEDIO POR TIPO DE VEH√çCULO (usando RDD):")
print("-" * 80)
edad_vehiculo = rdd \
    .map(lambda r: (r.TIPO_VEHICULO, (r.EDAD, 1))) \
    .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1])) \
    .mapValues(lambda x: round(x[0] / x[1], 1)) \
    .collect()

for vehiculo, edad_prom in sorted(edad_vehiculo, key=lambda x: x[1], reverse=True):
    print(f"  ‚Ä¢ {vehiculo}: {edad_prom} a√±os")

print("\n[5/5] Guardando resultados...")

# Crear carpeta de resultados si no existe
import os
os.makedirs("resultados", exist_ok=True)

# Guardar an√°lisis principales
top_localidades.write.mode("overwrite").csv(
    "resultados/top_localidades", 
    header=True
)
print("‚úì Top localidades guardado")

# Guardar datos procesados en Parquet
df.write.mode("overwrite").parquet("resultados/datos_procesados")
print("‚úì Datos procesados guardados en Parquet")

# Resumen ejecutivo
resumen = spark.sql("""
    SELECT 
        COUNT(*) as total_accidentes,
        SUM(NUM_HERIDOS) as total_heridos,
        SUM(NUM_MUERTOS) as total_muertos,
        SUM(TOTAL_VICTIMAS) as total_victimas,
        ROUND(AVG(EDAD), 1) as edad_promedio,
        COUNT(DISTINCT LOCALIDAD) as localidades_afectadas
    FROM accidentes
""")

print("\n" + "="*80)
print("RESUMEN EJECUTIVO")
print("="*80)
resumen.show(truncate=False)

# Guardar resumen
resumen.write.mode("overwrite").csv("resultados/resumen_ejecutivo", header=True)
print("‚úì Resumen ejecutivo guardado")

print("\n" + "="*80)
print("‚úÖ PROCESAMIENTO BATCH COMPLETADO EXITOSAMENTE")
print("="*80)
print("\nArchivos generados:")
print("  1. resultados/top_localidades/ - Top localidades CSV")
print("  2. resultados/datos_procesados/ - Dataset en Parquet")
print("  3. resultados/resumen_ejecutivo/ - Resumen CSV")
print("\nüöÄ Listo para procesamiento streaming!")
print("="*80)

# Detener sesi√≥n Spark
spark.stop()
